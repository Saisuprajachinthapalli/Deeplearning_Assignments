{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQ8K_47V2Aj5"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "from keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import regularizers\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n",
        "    X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
        "    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "# shape of X_train and X_test\n",
        "\n",
        "print('shape of X_train and X_test')\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "\n",
        "# plot 4 images as gray scale\n",
        "\n",
        "plt.subplot(221)\n",
        "plt.imshow(X_train[0], cmap=plt.get_cmap('gray'))\n",
        "plt.subplot(222)\n",
        "plt.imshow(X_train[1], cmap=plt.get_cmap('gray'))\n",
        "plt.subplot(223)\n",
        "plt.imshow(X_train[2], cmap=plt.get_cmap('gray'))\n",
        "plt.subplot(224)\n",
        "plt.imshow(X_train[3], cmap=plt.get_cmap('gray'))\n",
        "\n",
        "# show the plot\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Show the pictures\n",
        "\n",
        "plt.subplot(221)\n",
        "plt.imshow(X_train[15], cmap=plt.get_cmap('gray'))\n",
        "plt.subplot(222)\n",
        "plt.imshow(X_train[20], cmap=plt.get_cmap('gray'))\n",
        "plt.subplot(223)\n",
        "plt.imshow(X_train[25], cmap=plt.get_cmap('gray'))\n",
        "plt.subplot(224)\n",
        "plt.imshow(X_train[30], cmap=plt.get_cmap('gray'))\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print('-------------------------')\n",
        "\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "\n",
        "# normalize inputs from 0-255 to 0-1\n",
        "\n",
        "X_train = X_train / 255\n",
        "X_test = X_test / 255\n",
        "\n",
        "num_classes = 10\n",
        "\n",
        "# one hot encode outputs\n",
        "\n",
        "Y_train = to_categorical(Y_train, num_classes)\n",
        "Y_test = to_categorical(Y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "\n",
        "# add batch normalization\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "\n",
        "# add batch normalization\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# add dropout\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "\n",
        "# add dropout\n",
        "\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# you may change epochs and batch_size for performance\n",
        "\n",
        "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=10, batch_size=200, verbose=2)\n",
        "\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "\n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "#print('Test loss:', score[0])\n",
        "\n",
        "print('Test loss:', score[0])\n",
        "\n",
        "print('-------------------------------------------------------------------------------------------')\n",
        "\n",
        "\n",
        "# 2) compare the performance of CNN with that of MLP, and explain the reason in your words.\n",
        "\n",
        "print(''' CNNs are specifically designed to process data that has a structure, such as images. They can extract features from different parts of the image and learn how these features relate to each other. This makes them very effective for tasks like image classification, where the goal is to identify the object in an image.\n",
        "MLPs, on the other hand, are not as well-suited for image-related tasks. They do not have the same ability to extract spatial patterns and relationships from images. As a result, they are not as effective for tasks like image classification. However, MLPs are still useful for a wide range of other tasks, such as tabular data analysis.\n",
        "In general, CNNs are the better choice for image-related tasks, while MultiLayer Perceptrons are the better choice for other types of tasks.''')\n",
        "\n",
        "\n",
        "print('-------------------------------------------------------------------------------')\n",
        "\n",
        "# 3) Change parameters and compare the performance\n",
        "\n",
        "# 3-1) change (filter_size) ‘kernel_size’ three times and compare the performance\n",
        "\n",
        "print('changing kernal_size 1st time')\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(5, 5),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "\n",
        "# add batch normalization\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "\n",
        "# add batch normalization\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# add dropout\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "\n",
        "# add dropout\n",
        "\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# you may change epochs and batch_size for performance\n",
        "\n",
        "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=10, batch_size=200, verbose=2)\n",
        "\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "\n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "#print('Test loss:', score[0])\n",
        "\n",
        "print('Test loss:', score[0])\n",
        "\n",
        "print('--------------------------------------------------------')\n",
        "\n",
        "print('changing kernal_size 2nd time')\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 4),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "\n",
        "# add batch normalization\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "\n",
        "# add batch normalization\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# add dropout\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "\n",
        "# add dropout\n",
        "\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# you may change epochs and batch_size for performance\n",
        "\n",
        "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=10, batch_size=200, verbose=2)\n",
        "\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "\n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "#print('Test loss:', score[0])\n",
        "\n",
        "print('Test loss:', score[0])\n",
        "\n",
        "print('---------------------------------------------------------')\n",
        "\n",
        "print('changing kernal_size 3rd time')\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(2, 2),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "\n",
        "# add batch normalization\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "\n",
        "# add batch normalization\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# add dropout\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "\n",
        "# add dropout\n",
        "\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# you may change epochs and batch_size for performance\n",
        "\n",
        "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=10, batch_size=200, verbose=2)\n",
        "\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "\n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "#print('Test loss:', score[0])\n",
        "\n",
        "print('Test loss:', score[0])\n",
        "\n",
        "\n",
        "# 3-2) add dropout right after MaxPooling and Dense hidden layer\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# add dropout\n",
        "\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "\n",
        "# add dropout\n",
        "\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# you may change epochs and batch_size for performance\n",
        "\n",
        "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=7, batch_size=100, verbose=2)\n",
        "\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "\n",
        "print('Test accuracy:', score[1])\n",
        "print('Test loss:', score[0])\n",
        "\n",
        "# 3-3) Add L2 regularization in Convolution layer and Dense layer, respectively as follows. Explain the effect of regularization in performance. Repeat this using L1 regularizarion\n",
        "\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', kernel_regularizer=regularizers.L2(l2=0.05), bias_regularizer=regularizers.L2(l2=0.05)))\n",
        "\n",
        "model.add(Dense(128, activation='relu', kernel_regularizer=regularizers.L2(l2=0.05),\n",
        "      bias_regularizer=regularizers.L2(l2=0.05)))\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "print(model.summary())\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# you may change epochs and batch_size for performance\n",
        "\n",
        "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=10, batch_size=200, verbose=2)\n",
        "\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "\n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "#print('Test loss:', score[0])\n",
        "\n",
        "print('Test loss:', score[0])\n",
        "\n",
        "print('Adding L1 regularization')\n",
        "\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu',\n",
        "      kernel_regularizer=regularizers.L1(l1=0.05),\n",
        "      bias_regularizer=regularizers.L1(l1=0.05)))\n",
        "\n",
        "model.add(Dense(128, activation='relu',\n",
        "      kernel_regularizer=regularizers.L1(l1=0.05),\n",
        "      bias_regularizer=regularizers.L1(l1=0.05)))\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "print(model.summary())\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# you may change epochs and batch_size for performance\n",
        "\n",
        "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=5, batch_size=200, verbose=2)\n",
        "\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "\n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "#print('Test loss:', score[0])\n",
        "\n",
        "print('Test loss:', score[0])\n",
        "\n",
        "print('-------------------------------------------------------------')\n",
        "\n",
        "# 3-4) Add batch normalization right after each Conv2D layer by adding the following line.\n",
        "\n",
        "from keras.layers import BatchNormalization\n",
        "\n",
        "model= Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "# add batch normalization\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "# add batch normalization\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# you may change epochs and batch_size for performance\n",
        "\n",
        "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=5, batch_size=100, verbose=2)\n",
        "\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "\n",
        "print('Test accuracy:', score[1])\n",
        "print('Test loss:', score[0])\n",
        "\n",
        "print('------------------------------------------------------')\n",
        "\n",
        "# 4)Add a few more convolutional layers and pooling layers. Explain the difference in performance.\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=5, batch_size=250, verbose=2)\n",
        "\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "\n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "print('Test loss:', score[0])\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}