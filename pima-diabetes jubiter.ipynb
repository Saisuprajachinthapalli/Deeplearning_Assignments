{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNlG1gGHfiiV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "columns=['Number of times pregnant',' Plasma glucose concentration a 2 hours in an oral glucose tolerance test',' Diastolic blood pressure','Triceps skin fold thickness','2-Hour serum insulin','Body mass index',' Diabetes pedigree function',' Age','Class variable']\n",
        "\n",
        "pima_data = pd.read_csv('/content/sample_data/pima-diabetes.csv',names=columns)\n",
        "\n",
        "# show dimension, datatype, and first 5 rows of pima_data.\n",
        "\n",
        "print(pima_data.info())\n",
        "\n",
        "print(pima_data.head())\n",
        "\n",
        "print(pima_data.tail())\n",
        "\n",
        "print(pima_data.shape)\n",
        "\n",
        "print(pima_data.dtypes)\n",
        "\n",
        "# for each attribute, show mean, count, std, min, max, etc\n",
        "\n",
        "print(pima_data.describe())\n",
        "\n",
        "# normalize every attribute (except target attribute) using MinMaxScaler\n",
        "\n",
        "print(\"MINMAX SCALER\")\n",
        "\n",
        "pima_data_norm = pima_data.drop('Class variable', axis=1)\n",
        "\n",
        "A = pima_data['Class variable']\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "df_scaled = scaler.fit_transform(pima_data_norm)\n",
        "\n",
        "print(df_scaled)\n",
        "\n",
        "# normalize every attribute (except target attribute) using StandardScaler\n",
        "\n",
        "print(\"STANDARD SCALER\")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "pima_data_norm = pima_data.drop('Class variable', axis=1)\n",
        "\n",
        "A = pima_data['Class variable']\n",
        "\n",
        "df_scaled = scaler.fit_transform(pima_data_norm)\n",
        "\n",
        "print(df_scaled)\n",
        "\n",
        "# With .pop() command, ‘class’ target attribute is extracted.\n",
        "# select input attributes without target attributes\n",
        "\n",
        "Y = pima_data.pop('Class variable')\n",
        "X = pima_data\n",
        "\n",
        "# split X, Y into X_train, X_test, Y_train, Y_test\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Show that split is correctly done\n",
        "\n",
        "print(X_train)\n",
        "print(Y_train)\n",
        "print(X_test)\n",
        "print(Y_test)\n",
        "\n",
        "# you can show the shape of each data & first 5 rows of each data\n",
        "\n",
        "print(X_train.shape)\n",
        "print(Y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(Y_test.shape)\n",
        "\n",
        "print(X_train.head())\n",
        "print(Y_train.head())\n",
        "print(X_test.head())\n",
        "print(Y_test.head())\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# In the following, use sigmoid activation function and define input_dim\n",
        "\n",
        "model.add(Dense(1, input_dim=8, activation='sigmoid'))\n",
        "\n",
        "# 1) use ‘adam’ optimizer, 2) loss function is binary_crossentropy\n",
        "# 3) metrics = accuracy\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics='accuracy')\n",
        "\n",
        "# change epoch values\n",
        "\n",
        "model.fit(X_train, Y_train, batch_size=20, epochs=40, validation_data=(X_test, Y_test))\n",
        "\n",
        "model.evaluate(X_train,Y_train)\n",
        "\n",
        "Y_predictions = model.predict(X_test)\n",
        "\n",
        "print('prediction: ', Y_predictions.flatten())\n",
        "print('Y_test: ', Y_test)\n",
        "\n",
        "# 2) Change epochs, batch_size, and see the changes in performance. Try at least FIVE different combinations\n",
        "\n",
        "print(\" 1st time changing epochs, batch_size\")\n",
        "\n",
        "model.fit(X_train, Y_train, batch_size=25, epochs=45, validation_data=(X_test, Y_test))\n",
        "\n",
        "model.evaluate(X_train,Y_train)\n",
        "\n",
        "Y_predictions = model.predict(X_test)\n",
        "\n",
        "print('prediction: ', Y_predictions.flatten())\n",
        "print('Y_test: ', Y_test)\n",
        "\n",
        "print(\" 2nd time changing epochs, batch_size\")\n",
        "\n",
        "model.fit(X_train, Y_train, batch_size=35, epochs=50, validation_data=(X_test, Y_test))\n",
        "\n",
        "model.evaluate(X_train,Y_train)\n",
        "\n",
        "Y_predictions = model.predict(X_test)\n",
        "\n",
        "print('prediction: ', Y_predictions.flatten())\n",
        "print('Y_test: ', Y_test)\n",
        "\n",
        "\n",
        "print(\" 3rd changing epochs, batch_size\")\n",
        "\n",
        "model.fit(X_train, Y_train, batch_size=40, epochs=55, validation_data=(X_test, Y_test))\n",
        "\n",
        "model.evaluate(X_train,Y_train)\n",
        "\n",
        "Y_predictions = model.predict(X_test)\n",
        "\n",
        "print('prediction: ', Y_predictions.flatten())\n",
        "print('Y_test: ', Y_test)\n",
        "\n",
        "\n",
        "print(\" 4th changing epochs, batch_size\")\n",
        "\n",
        "model.fit(X_train, Y_train, batch_size=45, epochs=60, validation_data=(X_test, Y_test))\n",
        "\n",
        "model.evaluate(X_train,Y_train)\n",
        "\n",
        "Y_predictions = model.predict(X_test)\n",
        "\n",
        "print('prediction: ', Y_predictions.flatten())\n",
        "print('Y_test: ', Y_test)\n",
        "\n",
        "print(\" 5th changing epochs, batch_size\")\n",
        "\n",
        "model.fit(X_train, Y_train, batch_size=50, epochs=65, validation_data=(X_test, Y_test))\n",
        "\n",
        "model.evaluate(X_train,Y_train)\n",
        "\n",
        "Y_predictions = model.predict(X_test)\n",
        "\n",
        "print('prediction: ', Y_predictions.flatten())\n",
        "print('Y_test: ', Y_test)\n",
        "\n",
        "\n",
        "# 3) Change error function to mean squared error, and explain the difference in performance\n",
        "print('changing error funtion to mean squared error loss = mse ')\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(1, input_dim=8, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse', metrics='accuracy')\n",
        "\n",
        "model.fit(X_train, Y_train, batch_size=30, epochs=50, validation_data=(X_test, Y_test))\n",
        "\n",
        "model.evaluate(X_train,Y_train)\n",
        "\n",
        "Y_predictions = model.predict(X_test)\n",
        "\n",
        "print('prediction: ', Y_predictions.flatten())\n",
        "print('Y_test: ', Y_test)\n"
      ]
    }
  ]
}